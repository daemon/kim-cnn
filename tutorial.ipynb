{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kim's CNN Model for Sentence Classification PyTorch Tutorial\n",
    "\n",
    "This tutorial guides new PyTorch users through implementing, training, and testing a simple CNN model for sentence classification. More specifically, users will implement the CNN model described in [this paper](http://emnlp2014.org/papers/pdf/EMNLP2014181.pdf) and use it on the [SST-1 dataset](https://nlp.stanford.edu/sentiment/), a popular dataset for sentence sentiment classification.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In learning the basics of PyTorch, I frequently consulted the [docs](http://pytorch.org/docs/master/). I found that most illuminating was using example code as reference and trying to write the project without copying and pasting. This way, I could understand the role of every function, instead of treating the entire model as opaque.\n",
    "\n",
    "The paper itself is simple to read and understand. However, it glosses over some details--mainly about preprocessing--such as the mechanics of preprocessing SST-1 and using a word2vec model. I will try to address as many of these details where possible.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "How would we translate the model to code? The paper mentions four different variants, but they differ mostly in only the input layer. This suggests that we modularize as follows:\n",
    "\n",
    "```\n",
    "input_module = rand | static | non-static | multi-channel\n",
    "full_model = conv_module(input_module)\n",
    "```\n",
    "\n",
    "In fact, upon closer examination, we realize that `rand`, `static`, and `non-static` have only a single channel and differ only in initialization and parameter update. Thus, we can further let `input_module = single-channel | multi-channel`.\n",
    "\n",
    "Here is the fully documented single-channel input module code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as nn_func\n",
    "\n",
    "\n",
    "class SingleChannelWordModel(nn.Module):\n",
    "    \"\"\"\n",
    "    The input layer module for rand, static, and non-static.\n",
    "    \"\"\"\n",
    "    def __init__(self, id_dict, weights, unknown_vocab=[], static=True):\n",
    "        \"\"\"\n",
    "        Creates the model.\n",
    "\n",
    "        Args:\n",
    "            id_dict: the id-to-word dictionary\n",
    "            weights: the word vector matrix that maps id to a word vector\n",
    "            unknown_vocab: the list of words that are not in id_dict. These will be randomly initialized.\n",
    "            static: if True, do not compute gradients and update weights.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        vocab_size = len(id_dict) + len(unknown_vocab)\n",
    "        self.n_channels = 1\n",
    "        self.lookup_table = id_dict\n",
    "\n",
    "        # Build and initialize the weight matrix, concatenating random vectors for unknown words\n",
    "        last_id = max(id_dict.values())\n",
    "        for word in unknown_vocab:\n",
    "            last_id += 1\n",
    "            self.lookup_table[word] = last_id\n",
    "        # Do the concatenation of random weights for unknown words\n",
    "        self.weights = np.concatenate((weights, np.random.rand(len(unknown_vocab), 300) - 0.5))\n",
    "        self.dim = self.weights.shape[1] # the size of the embedding\n",
    "\n",
    "        # The word embedding layer. Note that any nn.Module that __setattr__'s to another module becomes part of \n",
    "        # the other module. That is, self.embedding = nn.Embedding(..) effectively adds all of self.embedding's \n",
    "        # parameters to the self parameter list, since nn.Embedding subclasses nn.Module.\n",
    "        self.embedding = nn.Embedding(vocab_size, self.dim, padding_idx=2)\n",
    "        # Copy the weights to initialize the embedding\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(self.weights))\n",
    "\n",
    "        # Turn off gradient computation if static\n",
    "        if static:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "\n",
    "    @classmethod\n",
    "    def make_random_model(cls, id_dict, unknown_vocab=[], dim=300):\n",
    "        # Creates a rand model as specified in the paper. All weights are randomly initialized.\n",
    "        weights = np.random.rand(len(id_dict), dim) - 0.5\n",
    "        return cls(id_dict, weights, unknown_vocab, static=False)\n",
    "\n",
    "    # This function gets run on each forward pass of the model. x is the input tensor or variable.\n",
    "    def forward(self, x):\n",
    "        # self.embedding(x) looks up the embedding for x, where x is a tensor of indices.\n",
    "        # That is, if self.embedding.weights contains [[1, 2, 3], [4, 5, 6]], then self.embedding([0, 1, 0])\n",
    "        # returns [[1, 2, 3], [4, 5, 6], [1, 2, 3]].\n",
    "        batch = self.embedding(x)\n",
    "        # We expand the channel dimension, so batch has shape (batch, channel, sent length, embed dim)\n",
    "        return batch.unsqueeze(1)\n",
    "\n",
    "    # Converts each word in sentences to a suitable list of embedding indices.\n",
    "    def lookup(self, sentences):\n",
    "        indices_list = []\n",
    "        max_len = 0\n",
    "        for sentence in sentences:\n",
    "            indices = []\n",
    "            for word in str(sentence).split():\n",
    "                try:\n",
    "                    index = self.lookup_table[word]\n",
    "                    indices.append(index)\n",
    "                except KeyError:\n",
    "                    continue\n",
    "            indices_list.append(indices)\n",
    "            if len(indices) > max_len: # Find the maximum length of the sentence to pad to.\n",
    "                max_len = len(indices)\n",
    "        for indices in indices_list:\n",
    "            # Specify that all indices of value \"2\" to be zero-padded. This is made apparent by padding_idx=2 in\n",
    "            # nn.Embedding's constructor. There seems to be a bug with negative pad values...\n",
    "            indices.extend([2] * (max_len - len(indices))) \n",
    "        return indices_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clear that the `multi-channel` input module is just a pair of `static` and `non-static` input modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultiChannelWordModel(nn.Module):\n",
    "    \"\"\"\n",
    "    All *ChannelWordModels have attributes n_channels, dim, and lookup(sentences). They will be used\n",
    "    in the CNN module code later.\n",
    "    \"\"\"\n",
    "    def __init__(self, id_dict, weights, unknown_vocab=[]):\n",
    "        super().__init__()\n",
    "        self.n_channels = 2 # We have 2 channels now\n",
    "        self.non_static_model = SingleChannelWordModel(id_dict, weights, unknown_vocab, static=False) # Non-static\n",
    "        self.static_model = SingleChannelWordModel(id_dict, self.non_static_model.weights) # Static\n",
    "        self.dim = self.static_model.dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        # One input batch comes from the static model\n",
    "        batch1 = self.static_model(x)\n",
    "        # The other comes from non-static, as specified in paper\n",
    "        batch2 = self.non_static_model(x)\n",
    "        return torch.cat((batch1, batch2), dim=1) # Concatenate batch1 and batch2 along the channel dimension\n",
    "\n",
    "    def lookup(self, sentences):\n",
    "        # Delegate lookup to one of the child models\n",
    "        return self.static_model.lookup(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now build the CNN module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KimCNN(nn.Module):\n",
    "    def __init__(self, word_model, **config):\n",
    "        \"\"\"\n",
    "        Creates a CNN sentence classification module as described by Yoon Kim.\n",
    "\n",
    "        Args:\n",
    "            word_model: the input layer model to use. Can be rand, static, non-static, or multi-channel, as represented by\n",
    "                SingleChannelWordModel and MultiChannelWordModel.\n",
    "            **config: A dictionary of configuration settings. If blank, it defaults to those recommended in Kim's paper.\n",
    "        \"\"\"\n",
    "        # In PyTorch, we typically initialize all the trainable modules/layers in __init__ and then use them in forward()\n",
    "        super().__init__()\n",
    "        n_fmaps = config.get(\"n_feature_maps\", 100)\n",
    "        weight_lengths = config.get(\"weight_lengths\", [3, 4, 5]) # the sizes of the convolutional kernel\n",
    "        embedding_dim = word_model.dim\n",
    "\n",
    "        # By doing self.word_model = word_model, word_model is now a sub-module of KimCNN: all its parameters are now\n",
    "        # part of KimCNN's parameters.\n",
    "        self.word_model = word_model\n",
    "        n_c = word_model.n_channels\n",
    "\n",
    "        # The convolutional layers, 3 of 3x300, 4x300, 5x300 by default. (300 is the embedding size)\n",
    "        self.conv_layers = [nn.Conv2d(n_c, n_fmaps, (w, embedding_dim), padding=(w - 1, 0)) for w in weight_lengths]\n",
    "        for i, conv in enumerate(self.conv_layers):\n",
    "            self.add_module(\"conv{}\".format(i), conv) # since conv_layers is a list, we need to add the modules manually\n",
    "        self.dropout = nn.Dropout(config.get(\"dropout\", 0.5)) # a dropout layer\n",
    "        # Finally linearly combine all conv layers to form a logits output for softmax with cross entropy loss...\n",
    "        # There are 5 sentiments in SST by default: very negative, negative, neutral, positive, very positive\n",
    "        self.fc = nn.Linear(len(self.conv_layers) * n_fmaps, config.get(\"n_labels\", 5)) \n",
    "\n",
    "    def preprocess(self, sentences):\n",
    "        # Preprocess the string sentences for input to the model. In other words, takes a list of string sentences and outputs\n",
    "        # its embedding tensor representation\n",
    "        return torch.from_numpy(np.array(self.word_model.lookup(sentences)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Runs x through the current word input model, which is one of rand, static, non-static, and multi-channel\n",
    "        x = self.word_model(x) # shape: (batch, channel, sent length, embed dim)\n",
    "        # Perform convolution with rectified linear units as recommended in most papers\n",
    "        x = [nn_func.relu(conv(x)).squeeze(3) for conv in self.conv_layers] # squeeze(3) to get rid of the extraneous dimension\n",
    "        # max-pool over time as mentioned in the paper\n",
    "        x = [nn_func.max_pool1d(c, c.size(2)).squeeze(2) for c in x]\n",
    "        # Concatenate along the second dimension:\n",
    "        x = torch.cat(x, 1)\n",
    "        # Apply dropout\n",
    "        x = self.dropout(x)\n",
    "        # Return logits\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "The problem of constructing `id_dict`, `weights`, and `unknown_vocab` for the input models still remains. Conceptually, `id_dict` maps words to their indices in `weights`; for example, if `id_dict = {\"a\": 0, \"hello\": 1}`, then the rows corresponding to indices of `0` and `1` would contain the word embeddings for \"a\" and \"hello\", respectively. `unknown_vocab` is simply the list of absent words in `id_dict`. It's used for constructing random word embeddings.\n",
    "\n",
    "I've provided the `weights` and `id_dict` data in the **data** directory in my [repository](https://github.com/daemon/kim-cnn). In principle, `weights` can be any numpy array and `id_dict` can be any pickled dictionary, both with the aforementioned format.  The SST data is there as well; Peng has already preprocessed that data in [his implementation](https://github.com/Impavidity/kim_cnn), assigning the sentiment labels in the TSV files.\n",
    "\n",
    "Please clone my repository for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "def load_sst_sets(dirname=\"data\", fmt=\"stsa.fine.{}.tsv\"):\n",
    "    set_names = [\"phrases.train\", \"dev\", \"test\"] # the pre-determined set names for training/dev/test\n",
    "    def read_set(name):\n",
    "        data_set = []\n",
    "        with open(os.path.join(dirname, fmt.format(name))) as f:\n",
    "            for line in f.readlines():\n",
    "                sentiment, sentence = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "                data_set.append((sentiment, sentence))\n",
    "        return np.array(data_set) # data_set is of form [[0, \"hello world!\"], ...], i.e. label-to-sentence\n",
    "    return [read_set(name) for name in set_names]\n",
    "\n",
    "def load_embed_data(dirname=\"data\", weights_file=\"embed_weights.npy\", id_file=\"word_id.dat\"):\n",
    "    id_file = os.path.join(dirname, id_file)\n",
    "    weights_file = os.path.join(dirname, weights_file)\n",
    "    train_file = os.path.join(dirname, \"stsa.fine.phrases.train.tsv\")\n",
    "    with open(id_file, \"rb\") as f:\n",
    "        id_dict = pickle.load(f)\n",
    "    with open(weights_file, \"rb\") as f:\n",
    "        weights = np.load(f)\n",
    "    unk_vocab = set()\n",
    "    unk_vocab_list = []\n",
    "    with open(train_file) as f:\n",
    "        for line in f.readlines():\n",
    "            words = line.split(\"\\t\")[1].replace(\"\\n\", \"\").split()\n",
    "            for word in words:\n",
    "                if word not in id_dict and word not in unk_vocab:\n",
    "                    unk_vocab.add(word)\n",
    "                    unk_vocab_list.append(word)\n",
    "    return (id_dict, weights, unk_vocab_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a way to convert the dataset returned by `load_sst_sets` into something PyTorch can use (i.e. a variable or tensor):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_dataset(model, dataset):\n",
    "    model_in = dataset[:, 1].reshape(-1) # grab all the sentences\n",
    "    model_out = dataset[:, 0].flatten().astype(np.int) # grab all the output labels\n",
    "    model_out = torch.autograd.Variable(torch.from_numpy(model_out)).cuda() # .cuda() moves it to the GPU\n",
    "    model_in = model.preprocess(model_in) # turn the sentences into embeddings in a tensor\n",
    "    model_in = torch.autograd.Variable(model_in.cuda()) # move to GPU and turn into variable, which is needed for backprop\n",
    "    return (model_in, model_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to regularize the weights by scaling them to have L2 norm <= 3 as mentioned in the paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clip_weights(parameter, s=3):\n",
    "    norm = parameter.weight.data.norm()\n",
    "    if norm < s:\n",
    "        return\n",
    "    parameter.weight.data.mul_(s / norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can begin training and evaluating our model. The code should be fairly self-explanatory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "\n",
    "def main():\n",
    "    torch.cuda.set_device(1)\n",
    "    id_dict, weights, unk_vocab_list = load_embed_data()\n",
    "    # Uncomment one of the following for rand/static/non-static/multi-channel, respectively\n",
    "    #word_model = model.SingleChannelWordModel.make_random_model(id_dict, unk_vocab_list)\n",
    "    word_model = model.SingleChannelWordModel(id_dict, weights, unk_vocab_list)\n",
    "    #word_model = model.SingleChannelWordModel(id_dict, weights, unk_vocab_list, static=False)\n",
    "    #word_model = model.MultiChannelWordModel(id_dict, weights, unk_vocab_list)\n",
    "    word_model.cuda()\n",
    "    kcnn = model.KimCNN(word_model) # Create the model\n",
    "    kcnn.cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    parameters = filter(lambda p: p.requires_grad, kcnn.parameters()) # Only retrieve parameters that are affected\n",
    "    # Adadelta works well but results in non-deterministic results on the GPU. Please use Adam or SGD if you wish for that.\n",
    "    optimizer = torch.optim.Adadelta(parameters, lr=0.001, weight_decay=0.)\n",
    "\n",
    "    train_set, dev_set, test_set = load_sst_sets()\n",
    "    for epoch in range(30):\n",
    "        kcnn.train()\n",
    "        optimizer.zero_grad()\n",
    "        np.random.shuffle(train_set)\n",
    "        mbatch_size = 50\n",
    "        i = 0\n",
    "        while i + mbatch_size < len(train_set):\n",
    "            mbatch = train_set[i:i + mbatch_size]\n",
    "            train_in, train_out = convert_dataset(kcnn, mbatch)\n",
    "\n",
    "            scores = kcnn(train_in)\n",
    "            loss = criterion(scores, train_out)\n",
    "            loss.backward() # Computes the gradients on all pertinent variables\n",
    "            optimizer.step()\n",
    "            for conv_layer in kcnn.conv_layers:\n",
    "                clip_weights(conv_layer)\n",
    "            i += mbatch_size\n",
    "\n",
    "            if i % 3000 == 0:\n",
    "                kcnn.eval()\n",
    "                dev_in, dev_out = convert_dataset(kcnn, dev_set)\n",
    "                scores = kcnn(dev_in)\n",
    "                n_correct = (torch.max(scores, 1)[1].view(len(dev_set)).data == dev_out.data).sum()\n",
    "                accuracy = n_correct / len(dev_set)\n",
    "                print(\"Dev set accuracy: {}\".format(accuracy))\n",
    "                kcnn.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After choosing the best model based on dev set accuracy, you may run the model on the test set. Please refer to [README](https://github.com/daemon/kim-cnn) for similar results. They're 0.5-2 percentage points lower than those of the paper, but that's probably because I'm using a smaller word embedding model.\n",
    "\n",
    "Please feel free to extend anything in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
